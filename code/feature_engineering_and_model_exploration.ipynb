{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.uci\n",
    "import chess.pgn\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics\n",
    "import ast\n",
    "from random import shuffle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import rc\n",
    "import seaborn as sns \n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.style as style\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.font_manager import FontProperties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perf_df(event = 'Rated Classical game'):\n",
    "    df = pd.read_csv(\"/Users/tylerahlstrom/Desktop/GitHub/DI_proposal/data/stockfish_performances_DC_1_17.csv\")\n",
    "    df = df.drop(df[df.event != event].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_perfs(joint_perf_df):\n",
    "    new_headers = ['elo', 'chosen_evals', 'option_evals', 'opp_elo', 'win', 'acc_name']\n",
    "    split_df = pd.DataFrame(columns = new_headers)\n",
    "    for index, row in joint_perf_df.iterrows():\n",
    "        if len(row['result']) is 3:\n",
    "            split_df = split_df.append({'elo': row['elo_w'], 'chosen_evals' : row['chosen_moves_eval_w'], 'option_evals' : row['available_moves_eval_w'], 'opp_elo': row['elo_b'], 'result': row['result'][0], 'acc_name': row['acc_name_w']}, ignore_index=True)\n",
    "            split_df = split_df.append({'elo': row['elo_b'], 'chosen_evals' : row['chosen_moves_eval_b'], 'option_evals' : row['available_moves_eval_b'], 'opp_elo': row['elo_w'], 'result': row['result'][2], 'acc_name': row['acc_name_b']}, ignore_index=True)\n",
    "    return split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_list(df):\n",
    "    for index, row in df.iterrows():\n",
    "        row['chosen_evals'] = json.loads(row['chosen_evals'])\n",
    "        row['option_evals'] = json.loads(row['option_evals'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_chosen_moves(dict_of_move_dict): #e.g. {u'11': {u'move_rank': 2, u'cp_scor#\n",
    "    lol_of_moves = []\n",
    "    for key, d_move in dict_of_move_dict.items():\n",
    "        lol_of_moves.append([key, d_move])\n",
    "    lol_of_moves.sort(key=lambda x: int(x[0]))\n",
    "    return lol_of_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_list_of_available_moves(dict_of_options_dict): # e.g. {u'24': {u'd7e8': {u'cp_score': -674, u'mate_s...\n",
    "    lolol_of_options  = []\n",
    "    for key, d_options in dict_of_options_dict.items():\n",
    "        lol_of_options = []\n",
    "        for key2, d_option in d_options.items():\n",
    "            lol_of_options.append([key2, d_option])\n",
    "        lol_of_options.sort(key=lambda x: int(x[1]['rank']))\n",
    "    \n",
    "        lolol_of_options.append([key, lol_of_options])\n",
    "    lolol_of_options.sort(key=lambda x: int(x[0][0]))\n",
    "    return lolol_of_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_rank_percentiles(list_of_moves):\n",
    "    list_of_rank_percentiles = []\n",
    "    for move in list_of_moves:\n",
    "        rank = int(move[1]['move_rank'])\n",
    "        num_options = int(move[1]['num_move_options'])\n",
    "        chunk = float(1)/float(num_options)\n",
    "        percentile = 1.0 - (float(rank-1) * chunk)\n",
    "        list_of_rank_percentiles.append(percentile)\n",
    "    return list_of_rank_percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_move_cps(list_of_moves):\n",
    "    list_of_cps = []\n",
    "    for move in list_of_moves:\n",
    "        cp = move[1]['cp_score']\n",
    "        list_of_cps.append(cp)\n",
    "    return list_of_cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_move_mates(list_of_moves):\n",
    "    list_of_mates = []\n",
    "    for move in list_of_moves:\n",
    "        mate = move[1]['mate_score']\n",
    "        list_of_mates.append(mate)\n",
    "    return list_of_mates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_option_cps(list_of_av_moves):\n",
    "    #print(list_of_av_moves)\n",
    "    lol_of_option_cps = []\n",
    "    for move in list_of_av_moves:\n",
    "        options_cps = []\n",
    "        for option in move[1]:\n",
    "            options_cps.append(option[1]['cp_score'])\n",
    "        lol_of_option_cps.append(options_cps)\n",
    "    \n",
    "    #print(lol_of_option_cps)\n",
    "    return lol_of_option_cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_dist_percentiles(move_cps, option_cps):\n",
    "    dist_scores = []\n",
    "    for i in range(len(move_cps)):\n",
    "        avg_cp = sum([x for x in option_cps[i] if x is not None])/float((len([x for x in option_cps[i] if x is not None])+0.1))\n",
    "        if move_cps[i] is None:\n",
    "            move_cps[i] = -2000\n",
    "        if avg_cp is None:\n",
    "            avg_cp = -300\n",
    "        comparison_cp = move_cps[i] - avg_cp\n",
    "        dist_scores.append(comparison_cp)\n",
    "        #percentile = float(better_than_cp)/min(float(total_cp), -1)\n",
    "        #dist_percentiles.append(percentile)\n",
    "    \n",
    "    return dist_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elo_system_prediction(result, opp_elo):\n",
    "    k_factor = 40\n",
    "    base_elo = 1560\n",
    "    Ea = 1./(1.+10.**((opp_elo - base_elo)/400.))\n",
    "    Rnew = base_elo + k_factor*(float(result) - Ea)\n",
    "    return Rnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data_df(event):\n",
    "    data_df = get_perf_df(event)\n",
    "    data_df = data_df.drop_duplicates()\n",
    "    data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "    data_df = split_perfs(data_df)\n",
    "    data_df = convert_json_to_list(data_df)\n",
    "    data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desired_data(complete_data_df, to_select = ['rank_percentiles', 'dist_percentiles', 'cps', 'result', 'opp_elo', 'acc_name', 'mates']):\n",
    "    i=0\n",
    "    X_selected_ldl = [] #X_selected_ldl is a list of dictionaries of lists, easiest way (i think) to trak all the relevant data\n",
    "    y = []#elo targets\n",
    "    for index, row in complete_data_df.iterrows():\n",
    "        if i == 0:\n",
    "            print(index, row)\n",
    "        i += 1\n",
    "        row_dict = {}\n",
    "        \n",
    "        ch_moves = get_list_of_chosen_moves(row['chosen_evals'])\n",
    "        av_moves = get_list_of_list_of_available_moves(row['option_evals'])\n",
    "        \n",
    "        if 'rank_percentiles' in to_select:\n",
    "            rank_percentiles = get_list_of_rank_percentiles(ch_moves)\n",
    "            row_dict['rank_percentiles'] = rank_percentiles\n",
    "        if 'cps' in to_select: #TO ADD: cp percentiles (e.g., just how much worse would the worst move have been?)\n",
    "            cps = get_list_of_move_cps(ch_moves)\n",
    "            row_dict['cps'] = cps\n",
    "        if 'dist_percentiles' in to_select:\n",
    "            option_cps = get_list_of_option_cps(av_moves)\n",
    "            dis_percentiles = get_list_of_dist_percentiles(cps, option_cps)\n",
    "            row_dict['dist_percentiles'] = dis_percentiles\n",
    "        if 'opp_elo' in to_select:\n",
    "            row_dict['opp_elo'] = row['opp_elo']\n",
    "        if 'result' in to_select:\n",
    "            row_dict['result'] = row['result']\n",
    "        if 'acc_name' in to_select:\n",
    "            row_dict['acc_name'] = row['acc_name']\n",
    "        if 'mates' in to_select:\n",
    "            mates = get_list_of_move_mates(ch_moves)\n",
    "            row_dict['mates'] = mates\n",
    "        \n",
    "        elo = row['elo']\n",
    "        X_selected_ldl.append(row_dict)\n",
    "        y_entry = []\n",
    "        y_entry.append(elo)\n",
    "        elo_system_prediction = get_elo_system_prediction(row_dict['result'],row_dict['opp_elo'])\n",
    "        y_entry.append(int(elo_system_prediction))\n",
    "        y.append(y_entry)\n",
    "\n",
    "    return X_selected_ldl, y #X_selected_ldl is a list of dictionaries of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_games(X, y):\n",
    "    i = 0\n",
    "    while i < (len(X)):\n",
    "        if len(X[i]['cps']) < 30:\n",
    "            X.pop(i)\n",
    "            y.pop(i)\n",
    "            i -= 1\n",
    "        i+=1\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X_input): #X should be at least length 30\n",
    "    \n",
    "    eg_cut = 7 #early game cutoff\n",
    "    mg_cut = 7\n",
    "    #eg_cut = ... the rest \n",
    "    \n",
    "    X = []\n",
    "    for i in range (len(X_input)):\n",
    "        X_entry = []\n",
    "        \n",
    "        #RANK\n",
    "        rank_list = X_input[i]['rank_percentiles'] \n",
    "        \n",
    "        rank_bestmove_score = sum(1 for percential in rank_list if percential == 1.0) / float(len(rank_list))\n",
    "        X_entry.append(rank_bestmove_score)\n",
    "        \n",
    "        rank_mistake_score = sum(1 for percential in rank_list if percential < 0.7 and percential > 0.5) / float(len(rank_list))\n",
    "        X_entry.append(rank_mistake_score)\n",
    "        \n",
    "        blunder_score_rank = sum(1 for percential in rank_list if percential < 0.5) / float(len(rank_list))\n",
    "        X_entry.append(blunder_score_rank) \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        avg_eg_rank = sum(rank_list[:eg_cut]) / eg_cut\n",
    "        X_entry.append(avg_eg_rank)\n",
    "        \n",
    "        avg_mg_rank = sum(rank_list[eg_cut:eg_cut+mg_cut]) / mg_cut\n",
    "        X_entry.append(avg_mg_rank)\n",
    "        \n",
    "        avg_lg_rank = sum(rank_list[eg_cut+mg_cut:]) / (len(rank_list) - (eg_cut+mg_cut))\n",
    "        X_entry.append(avg_lg_rank)\n",
    "        \n",
    "        ##CP\n",
    "        cp_list = X_input[i]['cps']\n",
    "        cp_list = [x for x in cp_list if x is not None]\n",
    "        \n",
    "        avg_cp = sum(cp_list) / len(cp_list)\n",
    "        X_entry.append(avg_cp)\n",
    "        \n",
    "        avg_mg_cp = sum(cp_list[eg_cut:eg_cut+mg_cut]) / mg_cut\n",
    "        X_entry.append(avg_mg_cp)\n",
    "        \n",
    "        std_cp = statistics.stdev(cp_list)\n",
    "        X_entry.append(std_cp)\n",
    "        \n",
    "        \n",
    "        #X_entry.append(99)\n",
    "        \n",
    "        #DIST\n",
    "        dist_list = X_input[i]['dist_percentiles']\n",
    "        dist_list = [x for x in dist_list if x is not None]\n",
    "        \n",
    "        avg_dist = sum(dist_list) / len(dist_list)\n",
    "        X_entry.append(avg_dist)\n",
    "        \n",
    "        avg_eg_dist = sum(dist_list[:eg_cut]) / eg_cut\n",
    "        X_entry.append(avg_eg_dist)\n",
    "        \n",
    "        avg_mg_dist = sum(dist_list[eg_cut:eg_cut+mg_cut]) / mg_cut\n",
    "        X_entry.append(avg_mg_dist)\n",
    "        \n",
    "        avg_lg_dist = sum(dist_list[eg_cut+mg_cut:]) / (len(dist_list) - (eg_cut+mg_cut))\n",
    "        X_entry.append(avg_lg_dist)\n",
    "\n",
    "        ##MATES\n",
    "        \n",
    "        mate_list = X_input[i]['mates']\n",
    "        \n",
    "        mate_blunders = sum(1 for each in mate_list if each and each[0:2] == 'DB')\n",
    "        X_entry.append(mate_blunders)\n",
    "        \n",
    "        mate_finds = sum(1 for each in mate_list if each and each[0:2] == 'AB')\n",
    "        X_entry.append(mate_finds)\n",
    "        \n",
    "        \n",
    "        ##BEST MOVES\n",
    "        rank_bestmove_score = sum(1 for percential in rank_list if percential == 1.0) / float(len(rank_list))\n",
    "        X_entry.append(rank_bestmove_score)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##LENGTH\n",
    "        X_entry.append(len(X_input[i]['rank_percentiles'])) ##or any of the sublists being used\n",
    "        \n",
    "        #OPPONENT AND RESULT\n",
    "        #X_entry.append(float(X_input[i]['opp_elo']))\n",
    "        X_entry.append(float(X_input[i]['result']) * 2 - 1)\n",
    "        \n",
    "        #PILE IT ON\n",
    "        for i in range(30):\n",
    "            X_entry.append(rank_list[i])\n",
    "        \n",
    "        \n",
    "        X.append(X_entry)\n",
    "    return X\n",
    "    #first 5, second 5, last all, average, standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_percentage = 10):\n",
    "    total = len(X)\n",
    "    \n",
    "    X, y = unison_shuffled_copies(X,y)\n",
    "    \n",
    "    train_size = int(total * (100-test_percentage)/100)\n",
    "\n",
    "    X_train = X[:train_size]\n",
    "    y_train_almost = y[:train_size]\n",
    "    y_train = np.array([z[0] for z in y_train_almost])\n",
    "\n",
    "    X_test = X[train_size:]\n",
    "    y_test_almost = y[train_size:]\n",
    "    y_test = np.array([z[0] for z in y_test_almost])\n",
    "    y_elo_sys_pred = np.array([z[1] for z in y_test_almost])\n",
    "\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y_elo_sys_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renorm_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(event = 'Rated Classical game'):\n",
    "    data_df = get_raw_data_df(event)\n",
    "    X, y = get_desired_data(data_df)\n",
    "    X, y = remove_short_games(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_raw = get_data(event = 'Rated Classical game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_num_moves = sum(len(entry['cps']) for entry in X_raw) / len(X_raw)\n",
    "print(\"Average number of moves: \" + str(avg_num_moves))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 12\n",
    "#print(X_raw[sample_num])\n",
    "X = np.array(extract_features(X_raw))\n",
    "#print(X[sample_num])\n",
    "y = np.array(y_raw)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, y_elo_sys_pred = split_data(X,y)\n",
    "print('Sample X, y: \\n{a} \\n{b}'.format(a = X_train[sample_num], b = y_train[sample_num]))\n",
    "print(\"\\nX_train: {a} \\nX_test: {b} \\ny_train: {c} \\ny_test: {d}\".format(a = X_train.shape, b = X_test.shape, c = y_train.shape, d = y_test.shape))\n",
    "X_train, X_test = renorm_data(X_train, X_test)\n",
    "\n",
    "print(\"Average elo: \" + str(sum(y_test) / len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"X_train: \" + str(X_train.shape))\n",
    "print (\"X_test: \" + str(X_test.shape))\n",
    "print (\"y_train: \" + str(y_train.shape))\n",
    "print (\"y_test: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(X_train[0])):\n",
    "#     X_plot = np.array([entry[i] for entry in X])\n",
    "#     slope, intercept, r_value, p_value, std_err = stats.linregress(y, X_plot)\n",
    "#     line = slope*y+intercept\n",
    "#     plt.plot(y,X_plot,'o', y, line)\n",
    "#     #np.poly1d(np.polyfit(X_plot, y, 1))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import datasets, tree, utils\n",
    "from sklearn import model_selection, ensemble\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from __future__ import print_function\n",
    "# import collections\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential, load_model\n",
    "# from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "# from keras.layers import LSTM\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# import numpy as np\n",
    "# import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "selector = SelectKBest(k = 'all')\n",
    "\n",
    "nn_reg = MLPRegressor(hidden_layer_sizes=(15,5,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=15000,\n",
    "        learning_rate_init=0.001,\n",
    "        alpha=0.000001,\n",
    "        early_stopping=True,\n",
    "        #shuffle=True,\n",
    "        warm_start=False)\n",
    "\n",
    "\n",
    "\n",
    "nn_est = Pipeline(steps=[('scaler', scaler),\n",
    "                         #('selector', selector),\n",
    "                         ('nn_reg', nn_reg)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_est.fit(X_train, y_train)\n",
    "y_pred = nn_est.predict(X_test)\n",
    "\n",
    "print(\"Training set r^2: \" + str(nn_est.score(X_train, y_train)))\n",
    "print(\"Test set r^2: \" + str(nn_est.score(X_test, y_test)))\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "selector = SelectKBest(k = 47)\n",
    "linaer_reg = LinearRegression()\n",
    "ridge_reg = Ridge(alpha = 24.0)\n",
    "\n",
    "linear_est = Pipeline(steps=[('scaler', scaler),\n",
    "                            ('selector', selector),\n",
    "                            ('ridge_reg', ridge_reg)])\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#     'selector__k': [x + 30 for x in range(19)],\n",
    "#     'ridge_reg__alpha': [x for x in range(25)],\n",
    "# }\n",
    "# search = GridSearchCV(linear_est, param_grid, iid=False,\n",
    "#                       return_train_score=False, verbose = 10)\n",
    "# search.fit(X_train, y_train)\n",
    "# print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "linear_est.fit(X_train, y_train)\n",
    "#linear_est.predict(X)\n",
    "#print(linear_est.score(X_train, y_train))\n",
    "print(linear_est.score(X_test, y_test))\n",
    "#print(linaer_reg.coef_)\n",
    "y_pred = linear_est.predict(X_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg_null = LinearRegression()\n",
    "\n",
    "null_model = Pipeline(steps=[('linaer_reg', linear_reg_null)])\n",
    "\n",
    "X_null_train = [entry[-2:] for entry in X_train]\n",
    "X_null_test = [entry[-2:] for entry in X_test]\n",
    "\n",
    "null_model.fit(X_null_train, y_train)\n",
    "#linear_est.predict(X)\n",
    "null_model_pred = null_model.predict(X_null_test)\n",
    "print(null_model.score(X_null_test, y_test))\n",
    "print(linear_reg_null.coef_)\n",
    "y_pred_null = null_model.predict(X_null_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(y_test, y_pred_null))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation (vs. Elo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "def get_r2(y_test, y_pred):\n",
    "    xs = np.array(y_test, dtype=np.float64)\n",
    "    ys = np.array(y_pred, dtype=np.float64)\n",
    "\n",
    "    def best_fit_slope_and_intercept(xs,ys):\n",
    "        m = (((mean(xs)*mean(ys)) - mean(xs*ys)) /\n",
    "             ((mean(xs)*mean(xs)) - mean(xs*xs)))\n",
    "        b = mean(ys) - m*mean(xs)\n",
    "        return m, b\n",
    "\n",
    "    def squared_error(ys_orig,ys_line):\n",
    "        return sum((ys_line - ys_orig) * (ys_line - ys_orig))\n",
    "\n",
    "    def coefficient_of_determination(ys_orig,ys_line):\n",
    "        y_mean_line = [mean(ys_orig) for y in ys_orig]\n",
    "        squared_error_regr = squared_error(ys_orig, ys_line)\n",
    "        squared_error_y_mean = squared_error(ys_orig, y_mean_line)\n",
    "        return 1 - (squared_error_regr/squared_error_y_mean)\n",
    "\n",
    "    m, b = best_fit_slope_and_intercept(xs,ys)\n",
    "    regression_line = [(m*x)+b for x in xs]\n",
    "\n",
    "    r_squared = coefficient_of_determination(ys,regression_line)\n",
    "    return (r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_est.predict(X_test)\n",
    "model_error = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "elo_sys_error = math.sqrt(mean_squared_error(y_test, y_elo_sys_pred))\n",
    "\n",
    "\n",
    "print(\"Average model error: \" + str(model_error))\n",
    "print(\"Average elo system error: \" + str(elo_sys_error))\n",
    "print(\"percentage improvement: \" + str('%.2f' % ((get_r2(y_test, y_pred) - get_r2(y_test, y_elo_sys_pred))\n",
    "                                                  /get_r2(y_test, y_elo_sys_pred) * 100) + '%'))\n",
    "print(\"model r2: \" + str((get_r2(y_test, y_pred))))\n",
    "print(\"standard elo r2: \" + str((get_r2(y_test, y_elo_sys_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import rc\n",
    "import seaborn as sns \n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.style as style\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi'] = 144\n",
    "style.use('seaborn-talk') \n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.5,)\n",
    "font = {'fontname':'Adobe Hebrew'}\n",
    "\n",
    "plt.minorticks_off()\n",
    "\n",
    "# print(y_pred)\n",
    "# print(y_elo_sys_pred)\n",
    "plt = sns.distplot(y_elo_sys_pred, hist = False, kde=True, color = 'red', kde_kws = {'shade': True, 'linewidth': 1}, label = 'standard system predictions')\n",
    "plt = sns.distplot(y_pred, hist = False, kde=True, color = 'blue',kde_kws = {'shade': True, 'linewidth': 1}, label = 'my model predictions')\n",
    "plt = sns.distplot(list(y_test), hist = False, kde=True, color = 'yellow',kde_kws = {'shade': True, 'linewidth': 1}, label = 'target values')\n",
    "plt.legend(prop={'size': 8})\n",
    "\n",
    "x_ticks_int = [800, 1000, 1200, 1400, 1600, 1800, 2000, 2200]\n",
    "x_ticks_str = [str(x) for x in x_ticks_int]\n",
    "    \n",
    "#plt.xticks(x_ticks_int, x_ticks_str, rotation='horizontal', fontsize=14, fontname = 'Lucida Console')\n",
    "print([x for x in y_pred])\n",
    "print([x for x in y_elo_sys_pred])\n",
    "print([x for x in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[0:1])\n",
    "print(X[0:1])\n",
    "#new_headers = ['elo', 'chosen_evals', 'option_evals', 'opp_elo', 'win']\n",
    "#    split_df = pd.DataFrame(columns = new_headers)\n",
    "\n",
    "x_df = pd.DataFrame({'% best move':X[:,0],'% mistakes':X[:,1], '% blunders':X[:,2], 'avg cp':X[:,3],\n",
    "                    'avg mid game strength':X[:,4], #'avg end game strength':X[:,5],'avg cp score':X[:,6], \n",
    "                    #'avg mid game cp score':X[:,4],\n",
    "                    'std cp score':X[:,5], 'avg strength above mean':X[:,6],\n",
    "                    'avg strength above mean (early game)':X[:,7], #'avg strength above mean (mid game)':X[:,10],\n",
    "                    #'std cp score (end game)':X[:,12]})\n",
    "                     'mate blunders':X[:,10],'mate finds':X[:,12]})\n",
    "y_df = pd.DataFrame({'elo':[e[0] for e in y]})\n",
    "#data = np.concatenate(y, X, ...), axis=1)\n",
    "# for i in range(len(y)):\n",
    "#     data.append([X[i].insert(0, y[i])])\n",
    "\n",
    "\n",
    "x_npdata = x_df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = min_max_scaler.fit_transform(x_npdata)\n",
    "x_df = pd.DataFrame(data_scaled, columns = x_df.columns)\n",
    "y_df['elo'] = pd.cut(y_df['elo'], [800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400], labels=[800, 1000, 1200, 1400, 1600, 1800, 2000, 2200])\n",
    "df = pd.concat([y_df, x_df], axis=1)\n",
    "df = df.dropna()\n",
    "#df2 = df[['elo','% mistakes']].copy()\n",
    "#ax = df2.plot.bar(rot=0)\n",
    "df = df.groupby(['elo']).mean()\n",
    "print(df)\n",
    "df.to_pickle('feature_df')\n",
    "# n = df.columns['elo']\n",
    "\n",
    "# # Drop that column\n",
    "# df.drop(n, axis = 1, inplace = True)\n",
    "\n",
    "# new_index = pd.Series([800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400])\n",
    "# df[0] = new_index\n",
    "#df\n",
    "#ax = df.plot.bar(y='% mistakes', rot=0)\n",
    "#print(df)\n",
    "\n",
    "axes = df.plot.bar(rot=0, subplots=True)\n",
    "axes[1].legend(loc=2)  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    " \n",
    "# set height of bar\n",
    "bars1 = df['% best move'].values.tolist()\n",
    "bars1 = [x - 0.2 for x in bars1]\n",
    "bars2 = df['avg mid game strength'].values.tolist()\n",
    "bars2 = [x - 0.5 for x in bars2]\n",
    "\n",
    "bars3 = df['% blunders'].values.tolist()\n",
    "bars4 = df['std cp score'].values.tolist()\n",
    "bars4 = [(x - 0.1) *2 for x in bars4]\n",
    "#bars3 = [29, 3, 24, 25, 17]\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "#r3 = [x + barWidth for x in r2]\n",
    " \n",
    "# Make the plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(r1, bars1, color='darkblue', width=barWidth, edgecolor='white', label='% best move chosen')\n",
    "plt.bar(r2, bars2, color='darkgreen', width=barWidth, edgecolor='white', label='avg mid game choice rank')\n",
    "#plt.xlabel('', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], df.index.tolist(), size = 20)\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=True)\n",
    "plt.yticks([])\n",
    "plt.ylabel('values scaled for viewing', size = 18)\n",
    "plt.legend(loc = 2, prop={'size': 18})\n",
    "plt.margins(x=0.05)\n",
    "plt.title(\"Several features used to train my models\", fontweight='bold', size = 32)\n",
    "\n",
    "#plt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='var3')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(r1, bars3, color='darkred', width=barWidth, edgecolor='white', label='% blunder')\n",
    "plt.bar(r2, bars4, color='darkgoldenrod', width=barWidth, edgecolor='white', label='SD centipawn score')\n",
    "plt.legend(prop={'size': 18})\n",
    "\n",
    "#plt.xticks(ticks=None, labels=None)\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Elo bins', fontweight='bold', size = 24)\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], df.index.tolist(), size = 20)\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=True)\n",
    "plt.yticks([])\n",
    "plt.ylabel('values scaled for viewing', size = 18)\n",
    "plt.margins(x=0.05)\n",
    "\n",
    "\n",
    "# Create legend & Show graphic\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.astype(k)\n",
    "df.plot(x='% best move', y='elo', kind='bar') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = []\n",
    "\n",
    "num_bins = 6\n",
    "\n",
    "bins = np.linspace(1000, 2500, num_bins)\n",
    "\n",
    "for bin in range (num_bins):\n",
    "    group_data_point = []\n",
    "    group_data_point.append(bins[bin])\n",
    "    min=1000.0+(300*bin)\n",
    "    group_count = 0\n",
    "    group_mean = []\n",
    "    group_std = []\n",
    "    for point in data_points:\n",
    "        if (point[0].item() > min) & (point[0].item() < min + 300):\n",
    "            group_count +=1\n",
    "            if not math.isnan(float(point[4])):\n",
    "                group_mean.append(point[4])\n",
    "            if not math.isnan(float(point[5])):\n",
    "                group_std.append(point[5])\n",
    "    group_mean = np.mean(array(group_mean))\n",
    "    group_std = np.mean(array(group_std))\n",
    "\n",
    "    if math.isnan(float(group_mean)):\n",
    "        print(group_mean)\n",
    "    if math.isnan(float(group_std)):\n",
    "        print(group_std)\n",
    "\n",
    "    group_data_point.append(group_mean)\n",
    "    group_data_point.append(group_std)\n",
    "    group_data.append(group_data_point)\n",
    "\n",
    "binned_data = array(group_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "N = binned_data.shape[0]\n",
    "print(N)\n",
    "elos = [int(el[0]) for el in binned_data]\n",
    "means = [el[1] for el in binned_data]\n",
    "stds = [el[2] for el in binned_data]\n",
    "\n",
    "print(elos)\n",
    "print(means)\n",
    "print(stds)\n",
    "\n",
    "\n",
    "\n",
    "### scaling for viewing\n",
    "means = [el*0.8 for el in means]\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "ind = np.arange(N)  \n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, means, width, color='seagreen')\n",
    "rects2 = ax.bar(ind + width*.8, stds, width, color='darkslateblue')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Values (scaled for viewing)')\n",
    "ax.set_xlabel('Elo bins ')\n",
    "\n",
    "ax.set_title('Features of evaluated moves')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(elos)\n",
    "\n",
    "plt.tick_params(left='off', labelleft='off')\n",
    "\n",
    "ax.legend((rects1[0], rects2[0]), ('Average move strength', 'Standard deviation of move strength'))\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only set up to work when there is no scaling, sensitive to the data\n",
    "y_pred_null = np.array([a[-2] for a in X_test])\n",
    "print(r2_score(y_test, y_pred_null))\n",
    "\n",
    "elo_step = 50\n",
    "y_adjustment = np.array([(a[-1]*2-1)*elo_step for a in X_test])\n",
    "y_pred_null = y_pred_null + y_adjustment\n",
    "print(y_pred_null[0:10])\n",
    "print(r2_score(y_test, y_pred_null))\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(y_test, y_pred_null))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null test for predicting elo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_elo = np.sum(y_train)/len(y_train)\n",
    "print(\"average elo: \" + str(average_elo))\n",
    "\n",
    "l = len(y_pred)\n",
    "y_pred = []\n",
    "for i in range(l):\n",
    "    y_pred.append(average_elo)\n",
    "print(math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using my own stockfish analysis\n",
    "def get_new_elo_perfs(eval_time): \n",
    "\n",
    "    assert type(eval_time) is float, \"Eval_time is not a float: %r\" % name\n",
    "\n",
    "    time_str = \"def\"\n",
    "\n",
    "    if eval_time == 0.5:\n",
    "        time_str = \"halfsecond\"\n",
    "    if eval_time == 1.0:\n",
    "        time_str = \"onesecond\"\n",
    "    if eval_time == 3.0:\n",
    "        time_str = \"threeseconds\"\n",
    "\n",
    "    if time_str == \"def\":\n",
    "        print(\"Eval time not available\")\n",
    "        return null\n",
    "\n",
    "\n",
    "    elo_and_p = []\n",
    "    performances_file = \"/Users/tylerahlstrom/Documents/GitHub/DI_proposal/data/stockfish_performances_\" + time_str + \".txt\"\n",
    "\n",
    "    with open(performances_file, 'r') as f:\n",
    "        for line in f:\n",
    "            listobj = ast.literal_eval(line)\n",
    "            elo_and_p.append(listobj)\n",
    "            \n",
    "    #removing openings for now\n",
    "    for item in elo_and_p:\n",
    "        del item[1]\n",
    "    \n",
    "    #inserting win/loss from separate extraction\n",
    "    winloss = []\n",
    "    winloss_file_dir = \"/Users/tylerahlstrom/Documents/GitHub/DI_proposal/data/winloss.txt\"\n",
    "    winloss_file = open(winloss_file_dir)\n",
    "    filecontents = winloss_file.readlines()\n",
    "    for line in filecontents:\n",
    "        target_content = line.strip('\\n')\n",
    "        winloss.append(target_content)\n",
    "        \n",
    "    for i in range(len(elo_and_p)):\n",
    "        elo_and_p[i].insert(1,winloss[i])\n",
    "    \n",
    "    \n",
    "    return elo_and_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bugged_evals(provided_list):\n",
    "\n",
    "    zero_rows = []\n",
    "    i = 0\n",
    "    for row in provided_list: \n",
    "        i+=1\n",
    "        if row.count(0.0)>3:\n",
    "            zero_rows.append(i)\n",
    "    if (len(zero_rows) > 1):\n",
    "        print(\"Found many zeros in these rows: \" + str(zero_rows[1:40]))\n",
    "        new_perf = [row for row in provided_list if row.count(0.0) < 2]\n",
    "        new_perf = [row for row in new_perf if row[0] > 1]\n",
    "        print(\"...and removed those rows\")\n",
    "    else:\n",
    "        print(\"List is cleared of zeros bug\")\n",
    "        new_perf = provided_list\n",
    "    return new_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_games(provided_list):\n",
    "\n",
    "    threshold = 12\n",
    "    \n",
    "    nomove_rows = []\n",
    "    i = 0\n",
    "    for row in provided_list:\n",
    "        if len(row) < threshold:\n",
    "            nomove_rows.append(i)\n",
    "        i+=1\n",
    "    if (len(nomove_rows) > 0):\n",
    "        print(\"Found less than 3 moves these rows: \" + str(nomove_rows[:]))\n",
    "        new_list = [row for row in provided_list if len(row) >= threshold]\n",
    "        print(\"...and removed those rows\")\n",
    "    else:\n",
    "        print(\"List has no short games\")\n",
    "        new_list = provided_list\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and clean it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_and_performance = get_new_elo_perfs(3.0)\n",
    "\n",
    "elo_and_performance = elo_and_performance[:1000]\n",
    "print(\"Data points before clean-up: \" + str(len(elo_and_performance)))\n",
    "elo_and_performance = remove_bugged_evals(elo_and_performance)\n",
    "elo_and_performance = remove_short_games(elo_and_performance)\n",
    "print(\"Data points after clean-up: \" + str(len(elo_and_performance)))\n",
    "print(elo_and_performance[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = []\n",
    "\n",
    "for item in elo_and_performance:\n",
    "    dp = []\n",
    "    \n",
    "    #0\n",
    "    dp.append(int(item[0]))\n",
    "    \n",
    "    #1\n",
    "    dp.append(float(item[1]))\n",
    "\n",
    "    \n",
    "    arr = np.array([float(entry) for entry in item[2:]], dtype = float)\n",
    "    \n",
    "    average = np.mean(arr)\n",
    "    #2\n",
    "    dp.append(average)\n",
    "    #dp.append(np.square(average))\n",
    "    \n",
    "    std = np.std(arr)\n",
    "    #3\n",
    "    dp.append(std)\n",
    "    #dp.append(np.square(std))\n",
    "\n",
    "\n",
    "    ten_ave = np.mean(arr[0:9])\n",
    "    #4\n",
    "    dp.append(ten_ave)\n",
    "    #dp.append(np.square(ten_ave))\n",
    "\n",
    "    \n",
    "    std_ten = np.std(arr[0:9])\n",
    "    #5\n",
    "    dp.append(std_ten) \n",
    "    #dp.append(np.square(std_ten))\n",
    "\n",
    "    \n",
    "    #6\n",
    "    dp.append(len(item))\n",
    "    #dp.append(np.square(len(item)))\n",
    "\n",
    "    \n",
    "#    arr = np.sort(arr)\n",
    "#    ave_five_worst = np.mean(arr[0:4])\n",
    "    #ave_ten_worst = np.mean(arr[0:9])\n",
    "    \n",
    "\n",
    "    #7\n",
    "#    dp.append(ave_five_worst)\n",
    "#    dp.append(np.square(ave_five_worst))\n",
    "\n",
    "    #8\n",
    "#    dp.append(ave_ten_worst)\n",
    "#    dp.append(np.square(ave_ten_worst))\n",
    "\n",
    "#    dp.append(np.sum(arr[0:4]))\n",
    "#    dp.append(np.sum(arr[0:9]))\n",
    "\n",
    "    #print(arr)\n",
    "    \n",
    "    data_points.append(dp)\n",
    "\n",
    "data_points = array(data_points)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#np.random.shuffle(data_points)\n",
    "print(data_points.shape)\n",
    "print(data_points[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage = 5\n",
    "total = len(data_points)\n",
    "train_size = int(total * (100-test_percentage)/100)\n",
    "\n",
    "\n",
    "X_train = data_points[:train_size,1:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "print(X_train[3])\n",
    "\n",
    "Y_train_elo = data_points[:train_size,0]\n",
    "Y_train_wl = data_points[:train_size,1]\n",
    "\n",
    "\n",
    "\n",
    "X_test = data_points[train_size:,1:]\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_test_elo = data_points[train_size:,0]\n",
    "Y_test_wl = data_points[train_size:,1]\n",
    "\n",
    "print (\"X_train: \" + str(X_train.shape))\n",
    "print (\"X_test: \" + str(X_test.shape))\n",
    "print (\"Y_train: \" + str(Y_train_elo.shape))\n",
    "print (\"Y_test: \" + str(Y_test_elo.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(2,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=100000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.00000001,\n",
    "        alpha=0.0001,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MLPRegressor(hidden_layer_sizes=(4,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=100000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.000001,\n",
    "        alpha=0.00001,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLPRegressor(hidden_layer_sizes=(6,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=1000000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.00000001,\n",
    "        alpha=0.00001,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MLPRegressor(hidden_layer_sizes=(8,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=100000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.00001,\n",
    "        alpha=0.00001,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train_elo)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "print(\"Training set r^2: \" + str(model.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model.score(X_test, Y_test_elo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, Y_train_elo)\n",
    "Y_pred = model1.predict(X_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "print(\"Training set r^2: \" + str(model1.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model1.score(X_test, Y_test_elo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, Y_train_elo)\n",
    "Y_pred = model2.predict(X_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "print(\"Training set r^2: \" + str(model2.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model2.score(X_test, Y_test_elo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(X_train, Y_train_elo)\n",
    "Y_pred = model3.predict(X_test)\n",
    "Y2_pred = model3.predict(X_train)\n",
    "print(\"Average absolute error train: \" + str(math.sqrt(mean_squared_error(Y_train_elo, Y2_pred))))\n",
    "print(\"Average absolute error test: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "\n",
    "print(\"Training set r^2: \" + str(model3.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model3.score(X_test, Y_test_elo)))\n",
    "\n",
    "# loss_values = model3.loss_curve_\n",
    "# plt.plot(loss_values)\n",
    "# plt.show()\n",
    "\n",
    "# train_sizes, train_scores, test_scores = learning_curve(model3, X, Y, n_jobs=-1, cv=cv, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "\n",
    "# train_scores_mean = np.mean(train_scores, axis=1)\n",
    "# train_scores_std = np.std(train_scores, axis=1)\n",
    "# test_scores_mean = np.mean(test_scores, axis=1)\n",
    "# test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"RandomForestClassifier\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.xlabel(\"Training examples\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.gca().invert_yaxis()\n",
    "\n",
    "# # box-like grid\n",
    "# plt.grid()\n",
    "\n",
    "# # plot the std deviation as a transparent range at each training set size\n",
    "# plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "# plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "# # plot the average training and test score lines at each training set size\n",
    "# plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "# plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "# # sizes the window for readability and displays the plot\n",
    "# # shows error from 0 to 1.1\n",
    "# #plt.ylim(-.1,1.1)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, Y_train_elo)\n",
    "Y_pred = regr.predict(X_test)\n",
    "print(regr.coef_)\n",
    "print(math.sqrt(mean_squared_error(Y_test_elo, Y_pred)))\n",
    "print(r2_score(Y_test_elo, Y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null test for predicting elo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_elo = np.sum(Y_train_elo)/len(Y_train_elo)\n",
    "print(\"average elo: \" + str(average_elo))\n",
    "\n",
    "l = len(Y_pred)\n",
    "Y_pred = []\n",
    "for i in range(l):\n",
    "    Y_pred.append(average_elo)\n",
    "print(math.sqrt(mean_squared_error(Y_test_elo, Y_pred)))\n",
    "print(r2_score(Y_test_elo, Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run best model to predict win/loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, Y_train_wl)\n",
    "Y_pred = model1.predict(X_test)\n",
    "#print(model.coef_)\n",
    "print(math.sqrt(mean_squared_error(Y_test_wl, Y_pred)))\n",
    "print(model1.score(X_test, Y_test_wl)) \n",
    "print(r2_score(Y_test_wl, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below lies messy graphing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = data_points[:,0]\n",
    "# y = [:,1]\n",
    "\n",
    "data = np.array(data_points)\n",
    "\n",
    "x = data[:,0]\n",
    "y = data[:,13]\n",
    "# x = np.array(x)\n",
    "# y = np.array(y)\n",
    "\n",
    "color = 'r'\n",
    "scale = 1.0\n",
    "\n",
    "\n",
    "#plt.legend()\n",
    "plt.scatter(x,y, s=scale, c=color, marker = \",\", lw=0, alpha = 0.5, label = 'accounts with # games played: <15')\n",
    "#plt.scatter(x, y, c=\"g\", alpha=0.5, marker=r'$\\clubsuit$',label=\"Luck\")\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "plt.plot(x, slope*x +intercept, '-', c = color, label = 'slope = ' + '%1.2E' % slope)\n",
    "print(\"first order: \" + str(np.power(r_value, 2)))\n",
    "\n",
    "p = np.polyfit(x,y,2)\n",
    "yfit = np.polyval(p,x)\n",
    "yresid = y - yfit\n",
    "SSresid = sum(np.power(yresid,2))\n",
    "SStotal = (len(y)-1) * np.var(y)\n",
    "rsq = 1 - SSresid/SStotal\n",
    "print(\"second order: \" + str(rsq))\n",
    "\n",
    "p = np.polyfit(x,y,3)\n",
    "yfit = np.polyval(p,x)\n",
    "yresid = y - yfit\n",
    "SSresid = sum(np.power(yresid,2))\n",
    "SStotal = (len(y)-1) * np.var(y)\n",
    "rsq = 1 - SSresid/SStotal\n",
    "print(\"third order: \" + str(rsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "c = 'r'\n",
    "m = 'o'\n",
    "\n",
    "xs = [(item[1]) for item in data_points]\n",
    "ys = [(item[2]) for item in data_points]\n",
    "zs = [(item[0]) for item in data_points]\n",
    "\n",
    "ax.scatter(xs, ys, zs, c=c, marker=m)\n",
    "\n",
    "# for c, m, zlow, zhigh in [('r', 'o', -50, -25), ('b', '^', -30, -5)]:\n",
    "#     xs = randrange(n, 23, 32)\n",
    "#     ys = randrange(n, 0, 100)\n",
    "#     zs = randrange(n, zlow, zhigh)\n",
    "\n",
    "ax.set_xlabel('mean strength')\n",
    "ax.set_ylabel('std strength')\n",
    "ax.set_zlabel('Elo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
