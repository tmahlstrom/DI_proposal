{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.uci\n",
    "import chess.pgn\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import ast\n",
    "from random import shuffle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using raw evaluation data\n",
    "def get_old_elo_perfs():\n",
    "    game_data_path = \"/Users/tylerahlstrom/Documents/GitHub/DI_proposal/data/game_data.pgn\"\n",
    "    eval_csv_path = \"/Users/tylerahlstrom/Documents/GitHub/DI_proposal/data/stockfish_evals.csv\"\n",
    "\n",
    "    pgns = open(game_data_path)\n",
    "    eval_csv = open(eval_csv_path)\n",
    "\n",
    "    sf_evals_csv = csv.reader(eval_csv, delimiter=',')\n",
    "\n",
    "    evals = []\n",
    "    for item in sf_evals_csv:\n",
    "        item.pop(0)\n",
    "        item = str(item)[2:-2].split()\n",
    "        evals.append(item)\n",
    "    evals.pop(0)\n",
    "    \n",
    "    elo_and_performance = []\n",
    "\n",
    "    for i in range(49000):\n",
    "        pw = []\n",
    "        pb = []\n",
    "        current_game = chess.pgn.read_game(pgns)\n",
    "        try:\n",
    "            pw.append(float(current_game.headers[\"WhiteElo\"]))\n",
    "        except:\n",
    "            pw.append(0.0)\n",
    "        try:\n",
    "            pb.append(float(current_game.headers[\"BlackElo\"]))\n",
    "        except:\n",
    "            pb.append(0.0)\n",
    "        try:\n",
    "            pw.append(float(evals[i][0]))\n",
    "        except:\n",
    "            pw.append(0.0)\n",
    "        for j in range (len(evals[i])):\n",
    "            if j == 0:\n",
    "                continue\n",
    "            try:\n",
    "                move_value = float(evals[i][j]) - float(evals[i][j-1])\n",
    "            except:\n",
    "                move_value = 0.0\n",
    "            if j % 2 != 0: \n",
    "                pb.append(-move_value)\n",
    "            else:\n",
    "                pw.append(move_value)\n",
    "        elo_and_performance.append(pw)\n",
    "        elo_and_performance.append(pb)\n",
    "\n",
    "    elo_and_performance = array(elo_and_performance)\n",
    "    \n",
    "    \n",
    "    return elo_and_performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # straight from kaggle dataset\n",
    "# elo_and_performance = []\n",
    "\n",
    "# for i in range(49000):\n",
    "#     pw = []\n",
    "#     pb = []\n",
    "#     current_game = chess.pgn.read_game(pgns)\n",
    "#     try:\n",
    "#         pw.append(float(current_game.headers[\"WhiteElo\"]))\n",
    "#     except:\n",
    "#         pw.append(0.0)\n",
    "#     try:\n",
    "#         pb.append(float(current_game.headers[\"BlackElo\"]))\n",
    "#     except:\n",
    "#         pb.append(0.0)\n",
    "#     try:\n",
    "#         pw.append(float(evals[i][0]))\n",
    "#     except:\n",
    "#         pw.append(0.0)\n",
    "#     for j in range (len(evals[i])):\n",
    "#         if j == 0:\n",
    "#             continue\n",
    "#         try:\n",
    "#             move_value = float(evals[i][j]) - float(evals[i][j-1])\n",
    "#         except:\n",
    "#             move_value = 0.0\n",
    "#         if j % 2 != 0: \n",
    "#             pb.append(-move_value)\n",
    "#         else:\n",
    "#             pw.append(move_value)\n",
    "#     elo_and_performance.append(pw)\n",
    "#     elo_and_performance.append(pb)\n",
    "\n",
    "# elo_and_performance = array(elo_and_performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using my own stockfish analysis\n",
    "def get_new_elo_perfs(eval_time): \n",
    "\n",
    "    assert type(eval_time) is float, \"Eval_time is not a float: %r\" % name\n",
    "\n",
    "    time_str = \"def\"\n",
    "\n",
    "    if eval_time == 0.5:\n",
    "        time_str = \"halfsecond\"\n",
    "    if eval_time == 1.0:\n",
    "        time_str = \"onesecond\"\n",
    "    if eval_time == 3.0:\n",
    "        time_str = \"threeseconds\"\n",
    "\n",
    "    if time_str == \"def\":\n",
    "        print(\"Eval time not available\")\n",
    "        return null\n",
    "\n",
    "\n",
    "    elo_and_p = []\n",
    "    performances_file = \"/Users/tylerahlstrom/Documents/GitHub/DI_proposal/data/stockfish_performances_\" + time_str + \".txt\"\n",
    "\n",
    "    with open(performances_file, 'r') as f:\n",
    "        for line in f:\n",
    "            listobj = ast.literal_eval(line)\n",
    "            elo_and_p.append(listobj)\n",
    "            \n",
    "    #removing openings for now\n",
    "    for item in elo_and_p:\n",
    "        del item[1]\n",
    "    \n",
    "    #inserting win/loss from separate extraction\n",
    "    winloss = []\n",
    "    winloss_file_dir = \"/Users/tylerahlstrom/Documents/GitHub/DI_proposal/data/winloss.txt\"\n",
    "    winloss_file = open(winloss_file_dir)\n",
    "    filecontents = winloss_file.readlines()\n",
    "    for line in filecontents:\n",
    "        target_content = line.strip('\\n')\n",
    "        winloss.append(target_content)\n",
    "        \n",
    "    for i in range(len(elo_and_p)):\n",
    "        elo_and_p[i].insert(1,winloss[i])\n",
    "    \n",
    "    \n",
    "    return elo_and_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bugged_evals(provided_list):\n",
    "\n",
    "    zero_rows = []\n",
    "    i = 0\n",
    "    for row in provided_list: \n",
    "        i+=1\n",
    "        if row.count(0.0)>3:\n",
    "            zero_rows.append(i)\n",
    "    if (len(zero_rows) > 1):\n",
    "        print(\"Found many zeros in these rows: \" + str(zero_rows[1:40]))\n",
    "        new_perf = [row for row in provided_list if row.count(0.0) < 2]\n",
    "        new_perf = [row for row in new_perf if row[0] > 1]\n",
    "        print(\"...and removed those rows\")\n",
    "    else:\n",
    "        print(\"List is cleared of zeros bug\")\n",
    "        new_perf = provided_list\n",
    "    return new_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_games(provided_list):\n",
    "\n",
    "    threshold = 12\n",
    "    \n",
    "    nomove_rows = []\n",
    "    i = 0\n",
    "    for row in provided_list:\n",
    "        if len(row) < threshold:\n",
    "            nomove_rows.append(i)\n",
    "        i+=1\n",
    "    if (len(nomove_rows) > 0):\n",
    "        print(\"Found less than 3 moves these rows: \" + str(nomove_rows[:]))\n",
    "        new_list = [row for row in provided_list if len(row) >= threshold]\n",
    "        print(\"...and removed those rows\")\n",
    "    else:\n",
    "        print(\"List has no short games\")\n",
    "        new_list = provided_list\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and clean it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_and_performance = get_new_elo_perfs(3.0)\n",
    "\n",
    "elo_and_performance = elo_and_performance[:1000]\n",
    "print(\"Data points before clean-up: \" + str(len(elo_and_performance)))\n",
    "elo_and_performance = remove_bugged_evals(elo_and_performance)\n",
    "elo_and_performance = remove_short_games(elo_and_performance)\n",
    "print(\"Data points after clean-up: \" + str(len(elo_and_performance)))\n",
    "print(elo_and_performance[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = []\n",
    "\n",
    "for item in elo_and_performance:\n",
    "    dp = []\n",
    "    \n",
    "    #0\n",
    "    dp.append(int(item[0]))\n",
    "    \n",
    "    #1\n",
    "    dp.append(float(item[1]))\n",
    "\n",
    "    \n",
    "    arr = np.array([float(entry) for entry in item[2:]], dtype = float)\n",
    "    \n",
    "    average = np.mean(arr)\n",
    "    #2\n",
    "    dp.append(average)\n",
    "    #dp.append(np.square(average))\n",
    "    \n",
    "    std = np.std(arr)\n",
    "    #3\n",
    "    dp.append(std)\n",
    "    #dp.append(np.square(std))\n",
    "\n",
    "\n",
    "    ten_ave = np.mean(arr[0:9])\n",
    "    #4\n",
    "    dp.append(ten_ave)\n",
    "    #dp.append(np.square(ten_ave))\n",
    "\n",
    "    \n",
    "    std_ten = np.std(arr[0:9])\n",
    "    #5\n",
    "    dp.append(std_ten) \n",
    "    #dp.append(np.square(std_ten))\n",
    "\n",
    "    \n",
    "    #6\n",
    "    dp.append(len(item))\n",
    "    #dp.append(np.square(len(item)))\n",
    "\n",
    "    \n",
    "#    arr = np.sort(arr)\n",
    "#    ave_five_worst = np.mean(arr[0:4])\n",
    "    #ave_ten_worst = np.mean(arr[0:9])\n",
    "    \n",
    "\n",
    "    #7\n",
    "#    dp.append(ave_five_worst)\n",
    "#    dp.append(np.square(ave_five_worst))\n",
    "\n",
    "    #8\n",
    "#    dp.append(ave_ten_worst)\n",
    "#    dp.append(np.square(ave_ten_worst))\n",
    "\n",
    "#    dp.append(np.sum(arr[0:4]))\n",
    "#    dp.append(np.sum(arr[0:9]))\n",
    "\n",
    "    #print(arr)\n",
    "    \n",
    "    data_points.append(dp)\n",
    "\n",
    "data_points = array(data_points)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#np.random.shuffle(data_points)\n",
    "print(data_points.shape)\n",
    "print(data_points[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percentage = 5\n",
    "total = len(data_points)\n",
    "train_size = int(total * (100-test_percentage)/100)\n",
    "\n",
    "\n",
    "X_train = data_points[:train_size,1:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "print(X_train[3])\n",
    "\n",
    "Y_train_elo = data_points[:train_size,0]\n",
    "Y_train_wl = data_points[:train_size,1]\n",
    "\n",
    "\n",
    "\n",
    "X_test = data_points[train_size:,1:]\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_test_elo = data_points[train_size:,0]\n",
    "Y_test_wl = data_points[train_size:,1]\n",
    "\n",
    "print (\"X_train: \" + str(X_train.shape))\n",
    "print (\"X_test: \" + str(X_test.shape))\n",
    "print (\"Y_train: \" + str(Y_train_elo.shape))\n",
    "print (\"Y_test: \" + str(Y_test_elo.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(2,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=100000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.00000001,\n",
    "        alpha=0.01,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MLPRegressor(hidden_layer_sizes=(4,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=100000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.000001,\n",
    "        alpha=0.00001,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLPRegressor(hidden_layer_sizes=(6,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=1000000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.00000001,\n",
    "        alpha=0.00001,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MLPRegressor(hidden_layer_sizes=(8,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        #learning_rate='adaptive',\n",
    "        max_iter=100000,\n",
    "        #learning_rate_init=0.001,\n",
    "        tol = 0.00001,\n",
    "        alpha=0.00001,\n",
    "        #early_stopping=False,\n",
    "        #shuffle=True,\n",
    "        warm_start=True,\n",
    "        random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train_elo)\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "print(\"Training set r^2: \" + str(model.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model.score(X_test, Y_test_elo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, Y_train_elo)\n",
    "Y_pred = model1.predict(X_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "print(\"Training set r^2: \" + str(model1.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model1.score(X_test, Y_test_elo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, Y_train_elo)\n",
    "Y_pred = model2.predict(X_test)\n",
    "print(\"Average absolute error: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "print(\"Training set r^2: \" + str(model2.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model2.score(X_test, Y_test_elo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(X_train, Y_train_elo)\n",
    "Y_pred = model3.predict(X_test)\n",
    "Y2_pred = model3.predict(X_train)\n",
    "print(\"Average absolute error train: \" + str(math.sqrt(mean_squared_error(Y_train_elo, Y2_pred))))\n",
    "print(\"Average absolute error test: \" + str(math.sqrt(mean_squared_error(Y_test_elo, Y_pred))))\n",
    "\n",
    "print(\"Training set r^2: \" + str(model3.score(X_train, Y_train_elo)))\n",
    "print(\"Test set r^2: \" + str(model3.score(X_test, Y_test_elo)))\n",
    "\n",
    "# loss_values = model3.loss_curve_\n",
    "# plt.plot(loss_values)\n",
    "# plt.show()\n",
    "\n",
    "# train_sizes, train_scores, test_scores = learning_curve(model3, X, Y, n_jobs=-1, cv=cv, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n",
    "\n",
    "# train_scores_mean = np.mean(train_scores, axis=1)\n",
    "# train_scores_std = np.std(train_scores, axis=1)\n",
    "# test_scores_mean = np.mean(test_scores, axis=1)\n",
    "# test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"RandomForestClassifier\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.xlabel(\"Training examples\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.gca().invert_yaxis()\n",
    "\n",
    "# # box-like grid\n",
    "# plt.grid()\n",
    "\n",
    "# # plot the std deviation as a transparent range at each training set size\n",
    "# plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "# plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "# # plot the average training and test score lines at each training set size\n",
    "# plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "# plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "# # sizes the window for readability and displays the plot\n",
    "# # shows error from 0 to 1.1\n",
    "# #plt.ylim(-.1,1.1)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, Y_train_elo)\n",
    "Y_pred = regr.predict(X_test)\n",
    "print(regr.coef_)\n",
    "print(math.sqrt(mean_squared_error(Y_test_elo, Y_pred)))\n",
    "print(r2_score(Y_test_elo, Y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null test for predicting elo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_elo = np.sum(Y_train_elo)/len(Y_train_elo)\n",
    "print(\"average elo: \" + str(average_elo))\n",
    "\n",
    "l = len(Y_pred)\n",
    "Y_pred = []\n",
    "for i in range(l):\n",
    "    Y_pred.append(average_elo)\n",
    "print(math.sqrt(mean_squared_error(Y_test_elo, Y_pred)))\n",
    "print(r2_score(Y_test_elo, Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run best model to predict win/loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, Y_train_wl)\n",
    "Y_pred = model1.predict(X_test)\n",
    "#print(model.coef_)\n",
    "print(math.sqrt(mean_squared_error(Y_test_wl, Y_pred)))\n",
    "print(model1.score(X_test, Y_test_wl)) \n",
    "print(r2_score(Y_test_wl, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below lies messy graphing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = []\n",
    "\n",
    "num_bins = 6\n",
    "\n",
    "bins = np.linspace(1000, 2500, num_bins)\n",
    "\n",
    "for bin in range (num_bins):\n",
    "    group_data_point = []\n",
    "    group_data_point.append(bins[bin])\n",
    "    min=1000.0+(300*bin)\n",
    "    group_count = 0\n",
    "    group_mean = []\n",
    "    group_std = []\n",
    "    for point in data_points:\n",
    "        if (point[0].item() > min) & (point[0].item() < min + 300):\n",
    "            group_count +=1\n",
    "            if not math.isnan(float(point[4])):\n",
    "                group_mean.append(point[4])\n",
    "            if not math.isnan(float(point[5])):\n",
    "                group_std.append(point[5])\n",
    "    group_mean = np.mean(array(group_mean))\n",
    "    group_std = np.mean(array(group_std))\n",
    "\n",
    "    if math.isnan(float(group_mean)):\n",
    "        print(group_mean)\n",
    "    if math.isnan(float(group_std)):\n",
    "        print(group_std)\n",
    "\n",
    "    group_data_point.append(group_mean)\n",
    "    group_data_point.append(group_std)\n",
    "    group_data.append(group_data_point)\n",
    "\n",
    "binned_data = array(group_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "N = binned_data.shape[0]\n",
    "print(N)\n",
    "elos = [int(el[0]) for el in binned_data]\n",
    "means = [el[1] for el in binned_data]\n",
    "stds = [el[2] for el in binned_data]\n",
    "\n",
    "print(elos)\n",
    "print(means)\n",
    "print(stds)\n",
    "\n",
    "\n",
    "\n",
    "### scaling for viewing\n",
    "means = [el*0.8 for el in means]\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "ind = np.arange(N)  \n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, means, width, color='seagreen')\n",
    "rects2 = ax.bar(ind + width*.8, stds, width, color='darkslateblue')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Values (scaled for viewing)')\n",
    "ax.set_xlabel('Elo bins ')\n",
    "\n",
    "ax.set_title('Features of evaluated moves')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(elos)\n",
    "\n",
    "plt.tick_params(left='off', labelleft='off')\n",
    "\n",
    "ax.legend((rects1[0], rects2[0]), ('Average move strength', 'Standard deviation of move strength'))\n",
    "\n",
    "\n",
    "plt.axhline(0, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = data_points[:,0]\n",
    "# y = [:,1]\n",
    "\n",
    "data = np.array(data_points)\n",
    "\n",
    "x = data[:,0]\n",
    "y = data[:,13]\n",
    "# x = np.array(x)\n",
    "# y = np.array(y)\n",
    "\n",
    "color = 'r'\n",
    "scale = 1.0\n",
    "\n",
    "\n",
    "#plt.legend()\n",
    "plt.scatter(x,y, s=scale, c=color, marker = \",\", lw=0, alpha = 0.5, label = 'accounts with # games played: <15')\n",
    "#plt.scatter(x, y, c=\"g\", alpha=0.5, marker=r'$\\clubsuit$',label=\"Luck\")\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "plt.plot(x, slope*x +intercept, '-', c = color, label = 'slope = ' + '%1.2E' % slope)\n",
    "print(\"first order: \" + str(np.power(r_value, 2)))\n",
    "\n",
    "p = np.polyfit(x,y,2)\n",
    "yfit = np.polyval(p,x)\n",
    "yresid = y - yfit\n",
    "SSresid = sum(np.power(yresid,2))\n",
    "SStotal = (len(y)-1) * np.var(y)\n",
    "rsq = 1 - SSresid/SStotal\n",
    "print(\"second order: \" + str(rsq))\n",
    "\n",
    "p = np.polyfit(x,y,3)\n",
    "yfit = np.polyval(p,x)\n",
    "yresid = y - yfit\n",
    "SSresid = sum(np.power(yresid,2))\n",
    "SStotal = (len(y)-1) * np.var(y)\n",
    "rsq = 1 - SSresid/SStotal\n",
    "print(\"third order: \" + str(rsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "c = 'r'\n",
    "m = 'o'\n",
    "\n",
    "xs = [(item[1]) for item in data_points]\n",
    "ys = [(item[2]) for item in data_points]\n",
    "zs = [(item[0]) for item in data_points]\n",
    "\n",
    "ax.scatter(xs, ys, zs, c=c, marker=m)\n",
    "\n",
    "# for c, m, zlow, zhigh in [('r', 'o', -50, -25), ('b', '^', -30, -5)]:\n",
    "#     xs = randrange(n, 23, 32)\n",
    "#     ys = randrange(n, 0, 100)\n",
    "#     zs = randrange(n, zlow, zhigh)\n",
    "\n",
    "ax.set_xlabel('mean strength')\n",
    "ax.set_ylabel('std strength')\n",
    "ax.set_zlabel('Elo')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
